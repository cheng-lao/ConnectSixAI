
# from torch import nn
from torch.nn import functional as F
from torch import optim,cuda
from torch.nn import Module
import torch,json
from torch.optim.lr_scheduler import MultiStepLR
from torch.utils.data import DataLoader

from collections import namedtuple
from .PolicyValueNet import PolicyValueNet
from .ChessBoard import ChessBoard
from .AlphaZeroMCTS import AlphaZeroMCTS
from .SelfPlayDataset import SelfPlayDataset
import os

SelfPlayData = namedtuple(
    'SelfPlayData', ['piList', 'zList', 'FeaturePlanesList'])

class PolivyValueLoss(Module):
    def __init__(self):
        super().__init__()
        
    def forward(self,PHat, Value, Pi, zList):
        """_summary_

        Args:
            PHat (List): å¯¹æ•°åŠ¨ä½œæ¦‚ç‡å‘é‡ (N,boardlen**2)
            Value (int): å¯¹æ¯ä¸ªå±€é¢çš„ä¼°å€¼ (N,)
            Pi (_type_): `mcts`è®¡ç®—çš„å¾—åˆ°çš„æ¯ä¸ªåŠ¨ä½œçš„æ¦‚ç‡å‘é‡   (N,boardlen**2)
            zList (_type_): æœ€ç»ˆçš„æ¸¸æˆç»“æœç›¸å¯¹æ¯ä¸€ä¸ªç©å®¶çš„å¥–åŠ± (N,)

        Returns:
            _type_: _description_
        """
        value_loss = F.mse_loss(Value, zList) #å‡æ–¹è¯¯å·®å‡½æ•° è®¡ç®—ä¼°å€¼è¯¯å·®
        policy_loss = -torch.sum(Pi * PHat,dim=1).mean() #äº¤å‰ç†µå‡½æ•° è®¡ç®—ç­–ç•¥è¯¯å·® 
        return policy_loss + value_loss


class TrainModel:
    """è®­ç»ƒæ¨¡å‹"""
    
    def __init__ (self,boardlen=17,lr=0.1,SelfPlayCnt=10000,MCTSItersCnt=500,
                 FeaturePlanesCnt=4,batchsize=500,StartTrainSize=500,CheckFrequency=100,
                 TestGames=10,exploration=4,IsUsedGPU=False,IsSaveGame=False,*args, **kwargs) -> None:
        """_summary_

        Args:
            `boardlen` (int, optional): æ£‹ç›˜è¾¹é•¿é•¿åº¦. Defaults to 17.
            `lr` (float, optional): å­¦ä¹ ç‡. Defaults to 0.1.
            `SelfPlayCnt` (int, optional): è‡ªæˆ‘åšå¼ˆå±€æ•°. Defaults to 10000.
            `MCTSItersCnt` (int, optional): è’™ç‰¹å¡æ´›æ ‘æœç´¢æ¬¡æ•°. Defaults to 500.
            `FeaturePlanesCnt` (int, optional): ç‰¹å¾å¹³é¢ä¸ªæ•°. Defaults to 7.
            `batchsize` (int, optional): æ‰¹é‡å¤§å°. Defaults to 500.
            `StartTrainSize` (int, optional): å¼€å§‹è®­ç»ƒæ¨¡å‹æ—¶çš„æœ€å°æ•°æ®é›†å¤§å°. Defaults to 500.
            `CheckFrequency` (int, optional): æµ‹è¯•æ¨¡å‹çš„é¢‘ç‡. Defaults to 100.
            `TestGames` (int, optional): æµ‹è¯•æ¨¡å‹æ—¶ä¸å†å²æœ€ä¼˜æ¨¡å‹çš„æ¯”èµ›å±€æ•°. Defaults to 10.
            `exploration` (int, optional): æ¢ç´¢å¸¸æ•°. Defaults to 4.
            `IsUsedGPU` (bool, optional): æ˜¯å¦ä½¿ç”¨GPU. Defaults to True.
            `IsSaveGame` (bool, optional): æ˜¯å¦ä¿å­˜å­åšå¼ˆçš„æ£‹è°±. Defaults to False.
        """
        self.boardlen = boardlen
        self.lr = lr
        self.SelfPlayCnt = SelfPlayCnt
        self.MCTSItersCnt = MCTSItersCnt
        self.FeaturePlanesCnt = FeaturePlanesCnt
        self.batchsize = batchsize
        self.StartTrainSize = StartTrainSize
        self.CheckFrequency = CheckFrequency
        self.TestGames = TestGames
        self.exploration = exploration
        self.IsUsedGPU = IsUsedGPU
        self.IsSaveGame = IsSaveGame
        self.device = torch.device("cuda:0" if torch.cuda.is_available() and IsUsedGPU else "cpu")
        self.chessboard = ChessBoard(boardlen,FeaturePlanesCnt)

        self.net = self.__GetNet()
        self.mcts = AlphaZeroMCTS(self.net,self.exploration,self.MCTSItersCnt,self.exploration)
        
        #åˆ›å»ºä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
        self.optimizer = optim.Adam(
            self.net.parameters(),lr=self.lr,weight_decay=1e-4)
        
        self.criterion = PolivyValueLoss()
        self.lr_scheduler =   MultiStepLR(                          #TODO
            self.optimizer,milestones=[1500,4000],gamma=0.1)      
        
        #åˆ›å»ºæ•°æ®é›†
        self.dataset = SelfPlayDataset(BoardLen=boardlen)
        
        # è®°å½•æ•°æ®
        self.trainLosses = self.__load_data('log/train_losses.json')
        self.game = self.__load_data('log/games.json')
    
    def __GetNet(self):
        """è·å–ç½‘ç»œ"""
        
        if not os.path.exists("model"):
            os.mkdir("model")
        if not os.path.exists("model/history"):
            os.mkdir("model/history")
        
        
        best_model = "model/best.pth"
        historyModel = sorted([i for i in os.listdir("model/history") if i.startswith("model")])
        
        print("ğŸŒ å¼€å§‹åŠ è½½ç½‘ç»œ!")
        if len(historyModel) > 0:
            best_model = "model/history/" + historyModel[-1]
            #ä»å†å²æ¨¡å‹å½“ä¸­æŒ‘é€‰æ¢ä¸€ä¸ªæœ€å¥½çš„æ¨¡å‹
            net = torch.load(best_model)
            net.set_device(self.device)  #TODO
        else:
            net = PolicyValueNet(self.boardlen,self.FeaturePlanesCnt)
        print("ğŸŒˆ ç½‘ç»œåŠ è½½å®Œæˆ!")
        
        return net
    
    def train(self):
        """è®­ç»ƒæ¨¡å‹"""
        for i in range(self.SelfPlayCnt):
            print(f"ğŸŒ å¼€å§‹ç¬¬{i+1}æ¬¡è‡ªæˆ‘åšå¼ˆ!")
            self.dataset.append(self.__SelfPlay())
            
            if len(self.dataset) >= self.StartTrainSize:
                dataLoader = DataLoader(self.dataset,batch_size=self.batchsize,shuffle=True,drop_last=False)
                print("ğŸš½ å¼€å§‹è®­ç»ƒæ¨¡å‹!")
                
                self.net.train()
                feature_planes,pi,z = next(dataLoader)
                feature_planes = feature_planes.to(self.device)
                pi = pi.to(self.device)
                z = z.to(self.device)
                for epoch in range(5):
                    #å‰å‘ä¼ æ’­
                    PHat,Value = self.net(feature_planes)
                    #åå‘ä¼ æ’­
                    self.optimizer.zero_grad()
                    loss = self.criterion(PHat,Value,pi,z)  #è®¡ç®—æŸå¤±
                    loss.backward() #åå‘ä¼ æ’­
                    
                    self.optimizer.step()   #æ›´æ–°å‚æ•°
                    self.lr_scheduler.step()    #æ›´æ–°å­¦ä¹ ç‡
                self.trainLosses.append(loss.item())    #è®°å½•æŸå¤±

            if (i+1) % self.CheckFrequency == 0:
                self.__testModel()
            
    def __SelfPlay(self):
        """è‡ªæˆ‘åšå¼ˆä¸€å±€
        
        Returns:
        """
        #åˆå§‹åŒ–æ£‹ç›˜å’Œæ•°æ®å®¹å™¨
        self.net.eval()
        self.chessboard.ClearBoard()
        piList,FeaturePlanesList,players = [],[],[]
        actionList = []
        
        first = True
        #å¼€å§‹ä¸€å±€æ¸¸æˆ
        while True:
            if first:
                action,pi = self.mcts.GetAction(self.chessboard)
                action = [action]
                first = False
            else:
                action1,pi = self.mcts.GetAction(self.chessboard)
                action2,pi = self.mcts.GetAction(self.chessboard)
                action = [action1,action2]
            FeaturePlanesList.append(self.chessboard.GetFeaturePlanes())
            piList.append(pi)
            players.append(self.chessboard.CurrentPlayer)
            actionList.append(action)
            self.chessboard.DoAction(action)
            #åˆ¤æ–­æ¸¸æˆæ˜¯å¦ç»“æŸ:
            IsOver, winner = self.chessboard.IsGameOver()
            print("IsOver is ",IsOver)
            print("actions ", action)
            if IsOver:
                if winner is not None:
                    zList = [1 if i == winner else -1 for i in players]
                else:
                    zList = [0]*len(players)
                break
            
            #é‡ç½®æ ¹èŠ‚ç‚¹:
        self.mcts.ResetRoot()
            
            #è¿”å›æ•°æ®
        if self.IsSaveGame:
            self.game.append(actionList,players)
        
        selfplaydata = SelfPlayData(
            piList=piList,zList=zList,FeaturePlanesList=FeaturePlanesList)
        return selfplaydata
        
    
    
    def __testModel(self):
        os.makedirs("model",exist_ok=True)

        model = "model/best.pth"
        
        if not os.path.exists(model):
            torch.save(self.net,model)
            return 
        
        bestmodel = torch.load(model)
        bestmodel.eval()    #å†å²æœ€å¥½çš„æ¨¡å‹
        bestmodel.set_device(self.device)
        mcts = AlphaZeroMCTS(bestmodel,self.exploration,self.MCTSItersCnt,self.exploration)
        
        self.mcts.SetSelfPlay(False)
        self.net.eval() #å½“å‰çš„æ¨¡å‹
        
        print(" å¼€å§‹æ¯”è¾ƒæ¨¡å‹æ€§èƒ½!")
        
        WinCnt = 0
        for i in range(self.TestGames):
            self.chessboard.ClearBoard()
            self.mcts.ResetRoot()
            mcts.ResetRoot()
            first = True
            while True:
                IsOver,winner = self.__do_mcts_action(self.mcts)
                if IsOver:
                    Nwins += int(winner==ChessBoard.BLACK)
                    break
                
                isOver,winner = self.__do_mcts_action(mcts)
                if isOver:
                    break
            
            #å¦‚æœèƒœç‡æ˜¯50%ä»¥ä¸Šï¼Œå°±ä¿å­˜å½“å‰æ¨¡å‹
        if Nwins >= self.TestGames//2:
            torch.save(self.net,model)
            print("æ›´æ–°æœ€æ–°çš„æ¨¡å‹")
        else:
            print("ä¸æ›´æ–°æœ€æ–°çš„æ¨¡å‹")
        
        self.mcts.IsSelfPlay(True)
    
    def save_model(self, model_name: str, loss_name: str, game_name: str):
        """ ä¿å­˜æ¨¡å‹

        Parameters
        ----------
        model_name: str
            æ¨¡å‹æ–‡ä»¶åç§°ï¼Œä¸åŒ…å«åç¼€

        loss_name: str
            æŸå¤±æ–‡ä»¶åç§°ï¼Œä¸åŒ…å«åç¼€

        game_name: str
            è‡ªå¯¹å¼ˆæ£‹è°±åç§°ï¼Œä¸åŒ…å«åç¼€
        """
        os.makedirs('model', exist_ok=True)

        path = f'model/{model_name}.pth'
        self.policy_value_net.eval()
        torch.save(self.policy_value_net, path)
        print(f'ğŸ‰ å·²å°†å½“å‰æ¨¡å‹ä¿å­˜åˆ° {os.path.join(os.getcwd(), path)}')

        # ä¿å­˜æ•°æ®
        with open(f'log/{loss_name}.json', 'w', encoding='utf-8') as f:
            json.dump(self.train_losses, f)

        if self.is_save_game:
            with open(f'log/{game_name}.json', 'w', encoding='utf-8') as f:
                json.dump(self.games, f)
    
    
    def __do_mcts_action(self, mcts: AlphaZeroMCTS):
        """ è·å–åŠ¨ä½œ """
        action = mcts.GetAction(self.chessboard)
        self.chessboard.DoAction(action)
        Isover, winner = self.chessboard.IsGameOver()
        return Isover, winner
    
    def __load_data(self, path: str):
        """ è½½å…¥å†å²æŸå¤±æ•°æ® """
        data = []
        try:
            with open(path, encoding='utf-8') as f:
                data = json.load(f)
        except:
            os.makedirs('log', exist_ok=True)

        return data
    